{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20f395-b21c-4a62-9d8d-c3c17a00319c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/karoldvl/ESC-50/archive/master.zip\n",
    "# !mkdir -p data && cd data && unzip ../master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dd4900-bbd6-41da-864a-c30a0a568f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:17837\n",
      "test_dataset:2894\n",
      "dataset info\n",
      "records: 17837\n",
      "classes: 2\n",
      "class[0] items: 11360\n",
      "class[1] items: 6477\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from utils.transform import UnsqueezeTransform, NoiseGeneratorTransform, FixedValueTransform, BinaryTransform\n",
    "from utils.dataset import dump_dataset_info\n",
    "from dataset.cry import CryDataset\n",
    "from dataset.noise import NoiseDataset\n",
    "\n",
    "\n",
    "# 定义超参数\n",
    "classes = 2\n",
    "train_batch_size = 200\n",
    "test_batch_size = 5\n",
    "train_data_esc50 = 'e:/dataset/out/esc50pp/training128mel1.pkl'\n",
    "valid_data_esc50 = 'e:/dataset/out/esc50pp/validation128mel1.pkl'\n",
    "train_data_donateacry = 'e:/dataset/out/dnac/donateacry.pkl'\n",
    "\n",
    "train_data_us8k = 'e:/dataset/out/us8k/training128mel1.pkl'\n",
    "valid_data_us8k = 'e:/dataset/out/us8k/validation128mel1.pkl'\n",
    "\n",
    "noise = NoiseGeneratorTransform(noise_std=1e-6, gamma=10, milestones=[10, 30, 60])\n",
    "\n",
    "def get_noise_dataset(shape=(1, 128, 256), target_id=0, num_samples=6000):\n",
    "    return NoiseDataset(shape, num_samples, target_id=target_id, noise_std=0.1)\n",
    "\n",
    "def get_donateacry_dataset():\n",
    "    return CryDataset(train_data_donateacry,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          # NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      target_transform=FixedValueTransform(value=1)\n",
    "                     )\n",
    "\n",
    "def get_esc50_train_dataset():\n",
    "    return CryDataset(train_data_esc50, \n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          # NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      # target_transform=ESC50LabelTransform()\n",
    "                      target_transform=BinaryTransform(20)\n",
    "                     )\n",
    "\n",
    "def get_esc50_valid_dataset():\n",
    "    return CryDataset(valid_data_esc50,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      #  target_transform=ESC50LabelTransform()\n",
    "                       target_transform=BinaryTransform(20)\n",
    "                     )\n",
    "\n",
    "def get_us8k_train_dataset():\n",
    "    return CryDataset(train_data_us8k, \n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                    #   target_transform=US8KLabelTransform()\n",
    "                     )\n",
    "\n",
    "def get_us8k_valid_dataset():\n",
    "    return CryDataset(valid_data_us8k,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                    #   target_transform=US8KLabelTransform()\n",
    "                     )\n",
    "def get_big_valid_dataset():\n",
    "    return ConcatDataset([get_esc50_valid_dataset(), get_us8k_valid_dataset()])\n",
    "\n",
    "train_loader = DataLoader(ConcatDataset([\n",
    "                                get_esc50_train_dataset(),\n",
    "                                get_donateacry_dataset(),\n",
    "                                # get_noise_dataset(num_samples=300)\n",
    "                            ]), \n",
    "                          batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(ConcatDataset([\n",
    "                                get_esc50_valid_dataset(),\n",
    "                            ]), \n",
    "                          batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "print(f'train_dataset:{len(train_loader.dataset)}')\n",
    "print(f'test_dataset:{len(test_loader.dataset)}')\n",
    "dump_dataset_info(train_loader.dataset)\n",
    "dump_dataset_info(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0555ac-f9d2-4360-a5f1-b4d5e7eca991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\condaenv\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120] LR:[0.001] Loss/train:3.0967 Loss/val:4886.3613% Accuracy/val:2.07%\n",
      "Epoch [2/120] LR:[0.001] Loss/train:2.4202 Loss/val:2575.6107% Accuracy/val:2.07%\n",
      "Epoch [3/120] LR:[0.001] Loss/train:1.8920 Loss/val:2821.1657% Accuracy/val:2.32%\n",
      "Epoch [4/120] LR:[0.001] Loss/train:1.5330 Loss/val:1694.4011% Accuracy/val:10.75%\n",
      "Epoch [5/120] LR:[0.001] Loss/train:1.3014 Loss/val:148.4905% Accuracy/val:49.07%\n",
      "Epoch [6/120] LR:[0.001] Loss/train:1.4458 Loss/val:347.1602% Accuracy/val:41.81%\n",
      "Epoch [7/120] LR:[0.001] Loss/train:1.1234 Loss/val:49.2483% Accuracy/val:97.27%\n",
      "Epoch [8/120] LR:[0.001] Loss/train:1.0683 Loss/val:12.8857% Accuracy/val:96.06%\n",
      "Epoch [9/120] LR:[0.001] Loss/train:0.8035 Loss/val:363.9656% Accuracy/val:54.22%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_010.pt\n",
      "Epoch [10/120] LR:[0.001] Loss/train:0.6908 Loss/val:13.5738% Accuracy/val:97.68%\n",
      "Epoch [11/120] LR:[0.001] Loss/train:0.9178 Loss/val:10.5904% Accuracy/val:97.79%\n",
      "Epoch [12/120] LR:[0.001] Loss/train:0.5620 Loss/val:96.1579% Accuracy/val:74.98%\n",
      "Epoch [13/120] LR:[0.001] Loss/train:0.5088 Loss/val:42.7605% Accuracy/val:90.32%\n",
      "Epoch [14/120] LR:[0.001] Loss/train:0.6763 Loss/val:79.0713% Accuracy/val:97.93%\n",
      "Epoch [15/120] LR:[0.001] Loss/train:0.6151 Loss/val:39.0152% Accuracy/val:98.31%\n",
      "Epoch [16/120] LR:[0.001] Loss/train:0.4049 Loss/val:121.5398% Accuracy/val:97.93%\n",
      "Epoch [17/120] LR:[0.001] Loss/train:0.5663 Loss/val:32.3782% Accuracy/val:97.96%\n",
      "Epoch [18/120] LR:[0.001] Loss/train:0.3116 Loss/val:1165.4419% Accuracy/val:18.73%\n",
      "Epoch [19/120] LR:[0.001] Loss/train:0.2846 Loss/val:24.3902% Accuracy/val:97.89%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_020.pt\n",
      "Epoch [20/120] LR:[0.001] Loss/train:0.3169 Loss/val:29.0493% Accuracy/val:98.27%\n",
      "Epoch [21/120] LR:[0.001] Loss/train:0.2326 Loss/val:30.0218% Accuracy/val:98.41%\n",
      "Epoch [22/120] LR:[0.001] Loss/train:0.3704 Loss/val:1168.7091% Accuracy/val:19.59%\n",
      "Epoch [23/120] LR:[0.001] Loss/train:0.2132 Loss/val:137.3254% Accuracy/val:77.37%\n",
      "Epoch [24/120] LR:[0.001] Loss/train:0.2428 Loss/val:445.8963% Accuracy/val:45.72%\n",
      "Epoch [25/120] LR:[0.001] Loss/train:0.2167 Loss/val:157.4953% Accuracy/val:97.93%\n",
      "Epoch [26/120] LR:[0.001] Loss/train:0.1661 Loss/val:38.7269% Accuracy/val:91.67%\n",
      "Epoch [27/120] LR:[0.001] Loss/train:0.4329 Loss/val:76.2711% Accuracy/val:83.97%\n",
      "Epoch [28/120] LR:[0.001] Loss/train:0.4105 Loss/val:26.9111% Accuracy/val:94.06%\n",
      "Epoch [29/120] LR:[0.001] Loss/train:0.1776 Loss/val:17.7272% Accuracy/val:98.17%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_030.pt\n",
      "Epoch [30/120] LR:[0.001] Loss/train:0.1981 Loss/val:30.9447% Accuracy/val:94.13%\n",
      "Epoch [31/120] LR:[0.0001] Loss/train:0.0834 Loss/val:21.7487% Accuracy/val:95.58%\n",
      "Epoch [32/120] LR:[0.0001] Loss/train:0.0558 Loss/val:12.1700% Accuracy/val:97.96%\n",
      "Epoch [33/120] LR:[0.0001] Loss/train:0.0644 Loss/val:12.4361% Accuracy/val:98.34%\n",
      "Epoch [34/120] LR:[0.0001] Loss/train:0.0763 Loss/val:11.3459% Accuracy/val:98.00%\n",
      "Epoch [35/120] LR:[0.0001] Loss/train:0.0590 Loss/val:15.3439% Accuracy/val:97.44%\n",
      "Epoch [36/120] LR:[0.0001] Loss/train:0.0506 Loss/val:15.1666% Accuracy/val:97.27%\n",
      "Epoch [37/120] LR:[0.0001] Loss/train:0.0509 Loss/val:15.7480% Accuracy/val:97.10%\n",
      "Epoch [38/120] LR:[0.0001] Loss/train:0.0475 Loss/val:11.9195% Accuracy/val:97.96%\n",
      "Epoch [39/120] LR:[0.0001] Loss/train:0.0506 Loss/val:13.2500% Accuracy/val:97.86%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_040.pt\n",
      "Epoch [40/120] LR:[0.0001] Loss/train:0.0465 Loss/val:15.4158% Accuracy/val:97.27%\n",
      "Epoch [41/120] LR:[0.0001] Loss/train:0.0417 Loss/val:13.2220% Accuracy/val:97.65%\n",
      "Epoch [42/120] LR:[0.0001] Loss/train:0.0496 Loss/val:14.4836% Accuracy/val:97.68%\n",
      "Epoch [43/120] LR:[0.0001] Loss/train:0.0376 Loss/val:19.6227% Accuracy/val:96.58%\n",
      "Epoch [44/120] LR:[0.0001] Loss/train:0.0314 Loss/val:17.7619% Accuracy/val:96.92%\n",
      "Epoch [45/120] LR:[0.0001] Loss/train:0.0306 Loss/val:20.8118% Accuracy/val:96.23%\n",
      "Epoch [46/120] LR:[0.0001] Loss/train:0.0399 Loss/val:20.7283% Accuracy/val:96.61%\n",
      "Epoch [47/120] LR:[0.0001] Loss/train:0.0524 Loss/val:16.2456% Accuracy/val:97.20%\n",
      "Epoch [48/120] LR:[0.0001] Loss/train:0.0503 Loss/val:19.6067% Accuracy/val:96.89%\n",
      "Epoch [49/120] LR:[0.0001] Loss/train:0.0382 Loss/val:19.5634% Accuracy/val:96.58%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_050.pt\n",
      "Epoch [50/120] LR:[0.0001] Loss/train:0.0331 Loss/val:17.2555% Accuracy/val:98.51%\n",
      "Epoch [51/120] LR:[0.0001] Loss/train:0.0292 Loss/val:15.2466% Accuracy/val:97.75%\n",
      "Epoch [52/120] LR:[0.0001] Loss/train:0.0621 Loss/val:19.9698% Accuracy/val:96.82%\n",
      "Epoch [53/120] LR:[0.0001] Loss/train:0.1030 Loss/val:13.3000% Accuracy/val:97.82%\n",
      "Epoch [54/120] LR:[0.0001] Loss/train:0.0886 Loss/val:24.8704% Accuracy/val:95.44%\n",
      "Epoch [55/120] LR:[0.0001] Loss/train:0.0502 Loss/val:13.7424% Accuracy/val:97.82%\n",
      "Epoch [56/120] LR:[0.0001] Loss/train:0.0504 Loss/val:14.7363% Accuracy/val:97.86%\n",
      "Epoch [57/120] LR:[0.0001] Loss/train:0.0622 Loss/val:17.1813% Accuracy/val:98.34%\n",
      "Epoch [58/120] LR:[0.0001] Loss/train:0.0923 Loss/val:13.8236% Accuracy/val:97.96%\n",
      "Epoch [59/120] LR:[0.0001] Loss/train:0.0573 Loss/val:14.6716% Accuracy/val:97.72%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_060.pt\n",
      "Epoch [60/120] LR:[0.0001] Loss/train:0.0759 Loss/val:36.0482% Accuracy/val:93.88%\n",
      "Epoch [61/120] LR:[1e-05] Loss/train:0.0492 Loss/val:15.4486% Accuracy/val:97.41%\n",
      "Epoch [62/120] LR:[1e-05] Loss/train:0.0386 Loss/val:16.4600% Accuracy/val:97.20%\n",
      "Epoch [63/120] LR:[1e-05] Loss/train:0.0396 Loss/val:14.4886% Accuracy/val:97.86%\n",
      "Epoch [64/120] LR:[1e-05] Loss/train:0.0379 Loss/val:15.7401% Accuracy/val:97.41%\n",
      "Epoch [65/120] LR:[1e-05] Loss/train:0.0328 Loss/val:15.8057% Accuracy/val:97.30%\n",
      "Epoch [66/120] LR:[1e-05] Loss/train:0.0364 Loss/val:14.6718% Accuracy/val:97.79%\n",
      "Epoch [67/120] LR:[1e-05] Loss/train:0.0592 Loss/val:15.8572% Accuracy/val:97.20%\n",
      "Epoch [68/120] LR:[1e-05] Loss/train:0.0388 Loss/val:16.3915% Accuracy/val:97.20%\n",
      "Epoch [69/120] LR:[1e-05] Loss/train:0.0289 Loss/val:16.6225% Accuracy/val:97.03%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_070.pt\n",
      "Epoch [70/120] LR:[1e-05] Loss/train:0.0347 Loss/val:16.0379% Accuracy/val:97.34%\n",
      "Epoch [71/120] LR:[1e-05] Loss/train:0.0377 Loss/val:15.5756% Accuracy/val:97.48%\n",
      "Epoch [72/120] LR:[1e-05] Loss/train:0.0312 Loss/val:15.5082% Accuracy/val:97.62%\n",
      "Epoch [73/120] LR:[1e-05] Loss/train:0.0272 Loss/val:15.7182% Accuracy/val:97.62%\n",
      "Epoch [74/120] LR:[1e-05] Loss/train:0.0437 Loss/val:16.1132% Accuracy/val:97.51%\n",
      "Epoch [75/120] LR:[1e-05] Loss/train:0.0287 Loss/val:14.9541% Accuracy/val:97.65%\n",
      "Epoch [76/120] LR:[1e-05] Loss/train:0.0301 Loss/val:16.2707% Accuracy/val:97.20%\n",
      "Epoch [77/120] LR:[1e-05] Loss/train:0.0327 Loss/val:15.5049% Accuracy/val:97.55%\n",
      "Epoch [78/120] LR:[1e-05] Loss/train:0.0295 Loss/val:17.3222% Accuracy/val:96.92%\n",
      "Epoch [79/120] LR:[1e-05] Loss/train:0.0258 Loss/val:16.5647% Accuracy/val:97.06%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_080.pt\n",
      "Epoch [80/120] LR:[1e-05] Loss/train:0.0313 Loss/val:15.8409% Accuracy/val:97.51%\n",
      "Epoch [81/120] LR:[1e-05] Loss/train:0.0288 Loss/val:17.3617% Accuracy/val:96.92%\n",
      "Epoch [82/120] LR:[1e-05] Loss/train:0.0307 Loss/val:15.6312% Accuracy/val:97.72%\n",
      "Epoch [83/120] LR:[1e-05] Loss/train:0.0262 Loss/val:16.5529% Accuracy/val:97.24%\n",
      "Epoch [84/120] LR:[1e-05] Loss/train:0.0326 Loss/val:15.7755% Accuracy/val:97.44%\n",
      "Epoch [85/120] LR:[1e-05] Loss/train:0.0288 Loss/val:16.7063% Accuracy/val:97.27%\n",
      "Epoch [86/120] LR:[1e-05] Loss/train:0.0270 Loss/val:16.3660% Accuracy/val:97.17%\n",
      "Epoch [87/120] LR:[1e-05] Loss/train:0.0299 Loss/val:15.4758% Accuracy/val:97.58%\n",
      "Epoch [88/120] LR:[1e-05] Loss/train:0.0522 Loss/val:17.4683% Accuracy/val:97.17%\n",
      "Epoch [89/120] LR:[1e-05] Loss/train:0.0339 Loss/val:16.0143% Accuracy/val:97.75%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_090.pt\n",
      "Epoch [90/120] LR:[1e-05] Loss/train:0.0282 Loss/val:15.7263% Accuracy/val:97.79%\n",
      "Epoch [91/120] LR:[1.0000000000000002e-06] Loss/train:0.0301 Loss/val:16.4582% Accuracy/val:97.51%\n",
      "Epoch [92/120] LR:[1.0000000000000002e-06] Loss/train:0.0349 Loss/val:17.4738% Accuracy/val:97.06%\n",
      "Epoch [93/120] LR:[1.0000000000000002e-06] Loss/train:0.0338 Loss/val:17.6507% Accuracy/val:97.03%\n",
      "Epoch [94/120] LR:[1.0000000000000002e-06] Loss/train:0.0213 Loss/val:16.3212% Accuracy/val:97.34%\n",
      "Epoch [95/120] LR:[1.0000000000000002e-06] Loss/train:0.0275 Loss/val:17.4198% Accuracy/val:97.20%\n",
      "Epoch [96/120] LR:[1.0000000000000002e-06] Loss/train:0.0331 Loss/val:17.3766% Accuracy/val:97.20%\n",
      "Epoch [97/120] LR:[1.0000000000000002e-06] Loss/train:0.0322 Loss/val:16.7129% Accuracy/val:97.20%\n",
      "Epoch [98/120] LR:[1.0000000000000002e-06] Loss/train:0.0306 Loss/val:16.8311% Accuracy/val:97.27%\n",
      "Epoch [99/120] LR:[1.0000000000000002e-06] Loss/train:0.0257 Loss/val:17.0210% Accuracy/val:97.24%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_100.pt\n",
      "Epoch [100/120] LR:[1.0000000000000002e-06] Loss/train:0.0237 Loss/val:16.6375% Accuracy/val:97.27%\n",
      "Epoch [101/120] LR:[1.0000000000000002e-06] Loss/train:0.0239 Loss/val:16.4225% Accuracy/val:97.34%\n",
      "Epoch [102/120] LR:[1.0000000000000002e-06] Loss/train:0.0261 Loss/val:16.8434% Accuracy/val:97.24%\n",
      "Epoch [103/120] LR:[1.0000000000000002e-06] Loss/train:0.0304 Loss/val:17.6818% Accuracy/val:97.10%\n",
      "Epoch [104/120] LR:[1.0000000000000002e-06] Loss/train:0.0950 Loss/val:16.5898% Accuracy/val:97.44%\n",
      "Epoch [105/120] LR:[1.0000000000000002e-06] Loss/train:0.0400 Loss/val:17.0143% Accuracy/val:97.37%\n",
      "Epoch [106/120] LR:[1.0000000000000002e-06] Loss/train:0.0257 Loss/val:17.5949% Accuracy/val:97.17%\n",
      "Epoch [107/120] LR:[1.0000000000000002e-06] Loss/train:0.0276 Loss/val:16.4143% Accuracy/val:97.44%\n",
      "Epoch [108/120] LR:[1.0000000000000002e-06] Loss/train:0.0356 Loss/val:17.5382% Accuracy/val:97.03%\n",
      "Epoch [109/120] LR:[1.0000000000000002e-06] Loss/train:0.0306 Loss/val:16.2335% Accuracy/val:97.55%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_110.pt\n",
      "Epoch [110/120] LR:[1.0000000000000002e-06] Loss/train:0.0319 Loss/val:16.7535% Accuracy/val:97.24%\n",
      "Epoch [111/120] LR:[1.0000000000000002e-06] Loss/train:0.0243 Loss/val:17.1775% Accuracy/val:97.24%\n",
      "Epoch [112/120] LR:[1.0000000000000002e-06] Loss/train:0.0261 Loss/val:17.3455% Accuracy/val:97.13%\n",
      "Epoch [113/120] LR:[1.0000000000000002e-06] Loss/train:0.0266 Loss/val:17.4806% Accuracy/val:97.10%\n",
      "Epoch [114/120] LR:[1.0000000000000002e-06] Loss/train:0.0221 Loss/val:17.4661% Accuracy/val:97.10%\n",
      "Epoch [115/120] LR:[1.0000000000000002e-06] Loss/train:0.0276 Loss/val:17.4913% Accuracy/val:97.24%\n",
      "Epoch [116/120] LR:[1.0000000000000002e-06] Loss/train:0.0269 Loss/val:16.8035% Accuracy/val:97.44%\n",
      "Epoch [117/120] LR:[1.0000000000000002e-06] Loss/train:0.0245 Loss/val:18.3914% Accuracy/val:96.79%\n",
      "Epoch [118/120] LR:[1.0000000000000002e-06] Loss/train:0.0263 Loss/val:16.3758% Accuracy/val:97.34%\n",
      "Epoch [119/120] LR:[1.0000000000000002e-06] Loss/train:0.0288 Loss/val:16.1204% Accuracy/val:97.58%\n",
      "saving model to: ./weights/mobilenetv3-binary/epoch_120.pt\n",
      "Epoch [120/120] LR:[1.0000000000000002e-06] Loss/train:0.0242 Loss/val:16.8649% Accuracy/val:97.37%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "# 混合精度\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystop import EarlyStopper\n",
    "from utils.modelsaver import ModelSaver\n",
    "from utils.focalloss import FocalLoss\n",
    "\n",
    "from model.mobilenet_v3 import MobileNetV3\n",
    "from model.mobilenet_v2 import MobileNetV2\n",
    "from train import train\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = MobileNetV2((1, 128, 100), 21).to(device)\n",
    "model = MobileNetV3((1, 128, 256), classes, width_multiplier=1.0, dropout_rate=0.2).to(device)\n",
    "# print(model)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "with_tqdm = False\n",
    "with_early_stop = False\n",
    "with_l2_regularization = True\n",
    "with_grad_scaler = True\n",
    "start_epoch = 0\n",
    "end_epoch = 120\n",
    "learning_rate = 1e-3\n",
    "save_model_start = 10\n",
    "save_model_period = 10\n",
    "l2_reg = 0.05\n",
    "\n",
    "model_name = f'mobilenetv3-binary'\n",
    "model_file = f'./weights/{model_name}/epoch_{{epoch:03d}}.pt'\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(classes, alpha=0.75, gamma=2.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
    "grad_scaler = GradScaler() if with_grad_scaler else None\n",
    "writer = SummaryWriter()\n",
    "\n",
    "if with_early_stop:\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "saver = ModelSaver(model, model_file, True, start_epoch, [10, 5], [])\n",
    "\n",
    "train(saver, criterion, scheduler, start_epoch, end_epoch, train_loader, test_loader, writer=writer, grad_scaler=grad_scaler, l2_reg=l2_reg)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69597e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "waveform, sr = torchaudio.load('E:/dataset/ESC-50-master/audio/1-100032-A-0.wav')\n",
    "resample = torchaudio.transforms.Resample(sr, 8000)\n",
    "spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=8000,\n",
    "    n_fft=512,\n",
    "    win_length=20,\n",
    "    hop_length=10, \n",
    "    n_mels=128)\n",
    "\n",
    "eps = torch.Tensor([1e-6])\n",
    "# spec = spec.numpy()\n",
    "# spec = np.log(spec + eps)\n",
    "\n",
    "\n",
    "out = spectrogram(resample(waveform[:44100]))\n",
    "out += eps\n",
    "out = out.log()\n",
    "# time = len(out[0]) * 1000 / 8000\n",
    "# print(time)\n",
    "# print(out.shape)\n",
    "\n",
    "plt.pcolormesh(out[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c66696-84b2-4d5b-a87b-6aa328fede55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def get_spec(waveform, sampling_rate=24000, n_fft=512, window_length=20, hop_length=10):\n",
    "\tspecs = []\n",
    "\twaveform = torch.Tensor(waveform)\n",
    "\ttransform = torchaudio.transforms.MelSpectrogram(\n",
    "\t\tsample_rate=sampling_rate, \n",
    "\t\tn_fft=n_fft, \n",
    "\t\twin_length=window_length, \n",
    "\t\thop_length=hop_length, \n",
    "\t\tn_mels=128)\n",
    "\tspec = transform(waveform)\n",
    "\teps = 1e-6\n",
    "\tspec = spec.numpy()\n",
    "\tspec = np.log(spec + eps)\n",
    "\tx_min = spec.min()\n",
    "\tx_max = spec.max()\n",
    "\tspec = (spec - x_min) / (x_max - x_min)\n",
    "\tfor j in range(0, spec.shape[1] - 51, 50):\n",
    "\t\tslice = spec[:, j:j+100]\n",
    "\t\t# print(f'slice shape: {slice.shape}, range: {j}:{j+100}')\n",
    "\t\tspecs.append(slice)\n",
    "\treturn specs\n",
    "\n",
    "def extract_spectrogram(values, clip, entries):\n",
    "\tfor data in entries:\n",
    "\n",
    "\t\tnum_channels = 2\n",
    "\t\twindow_sizes = [20, 40]\n",
    "\t\thop_sizes = [10, 20]\n",
    "\t\t# window_sizes = [20]\n",
    "\t\t# hop_sizes = [10]\n",
    "\n",
    "\t\tspecs = []\n",
    "\t\tfor i in range(num_channels):\n",
    "\t\t\twindow_length = int(round(window_sizes[i]*args.sampling_rate/1000))\n",
    "\t\t\thop_length = int(round(hop_sizes[i]*args.sampling_rate/1000))\n",
    "\n",
    "\t\t\tclip = torch.Tensor(clip)\n",
    "\t\t\tspec = torchaudio.transforms.MelSpectrogram(sample_rate=args.sampling_rate, n_fft=512, win_length=window_length, hop_length=hop_length, n_mels=128)(clip)\n",
    "\t\t\teps = 1e-6\n",
    "\t\t\tspec = spec.numpy()\n",
    "\t\t\tspec = np.log(spec + eps)\n",
    "\t\t\t# print(f'channel: {i} shape: {spec.shape}')\n",
    "\t\t\tfor j in range(0, spec.shape[1] - 51, 50):\n",
    "\t\t\t\tslice = spec[:, j:j+100]\n",
    "\t\t\t\t# print(f'slice shape: {slice.shape}, range: {j}:{j+100}')\n",
    "\t\t\t\tspecs.append(slice)\n",
    "\t\t\t# print(spec.shape)\n",
    "\t\t\t# spec = np.asarray(torchvision.transforms.Resize((128, 250))(Image.fromarray(spec)))\n",
    "\t\t\t# specs.append(spec)\n",
    "\t\tnew_entry = {}\n",
    "\t\t# new_entry[\"audio\"] = clip.numpy()\n",
    "\t\tnew_entry[\"values\"] = np.array(specs)\n",
    "\t\tnew_entry[\"target\"] = data[\"target\"]\n",
    "\t\tvalues.append(new_entry)\n",
    "\n",
    "clip, sr = librosa.load(\"d:\\\\code\\\\jupyter\\\\audio\\\\positive\\\\baby_cry_16bit_8k.wav\", sr=24000)\n",
    "# clip, sr = librosa.load(\"E:\\\\dataset\\\\bilibili\\\\cry2.m4s\", sr=24000)\n",
    "print(clip.shape, sr)\n",
    "clip = clip[:len(clip) // 1000 * 1000]\n",
    "print(clip.shape)\n",
    "# entries = audios.loc[audios[\"filename\"]==audio].to_dict(orient=\"records\")\n",
    "values = get_spec(clip, sampling_rate=sr)\n",
    "values = [np.expand_dims(value, 0) for value in values[:1000]]\n",
    "values = torch.Tensor(values).to(device)\n",
    "print(values.size())\n",
    "# print(len(values))\n",
    "# print(values[0].shape)\n",
    "\n",
    "predict = model(values).detach().cpu().numpy()\n",
    "predict = [np.argmax(p) for p in predict]\n",
    "print(predict)\n",
    "# print(torch.argmax(predict[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

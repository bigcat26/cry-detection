{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20f395-b21c-4a62-9d8d-c3c17a00319c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/karoldvl/ESC-50/archive/master.zip\n",
    "# !mkdir -p data && cd data && unzip ../master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dd4900-bbd6-41da-864a-c30a0a568f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:17837\n",
      "test_dataset:2894\n",
      "dataset info\n",
      "records: 17837\n",
      "classes: 2\n",
      "class[0] items: 11360\n",
      "class[1] items: 6477\n",
      "dataset info\n",
      "records: 2894\n",
      "classes: 2\n",
      "class[0] items: 2834\n",
      "class[1] items: 60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from utils.transform import UnsqueezeTransform, NoiseGeneratorTransform, FixedValueTransform, BinaryTransform\n",
    "from utils.dataset import dump_dataset_info\n",
    "from dataset.cry import CryDataset\n",
    "from dataset.noise import NoiseDataset\n",
    "\n",
    "\n",
    "# 定义超参数\n",
    "classes = 2\n",
    "train_batch_size = 200\n",
    "test_batch_size = 5\n",
    "train_data_esc50 = 'e:/dataset/out/esc50pp/training128mel1.pkl'\n",
    "valid_data_esc50 = 'e:/dataset/out/esc50pp/validation128mel1.pkl'\n",
    "train_data_donateacry = 'e:/dataset/out/dnac/donateacry.pkl'\n",
    "\n",
    "train_data_us8k = 'e:/dataset/out/us8k/training128mel1.pkl'\n",
    "valid_data_us8k = 'e:/dataset/out/us8k/validation128mel1.pkl'\n",
    "\n",
    "noise = NoiseGeneratorTransform(noise_std=1e-6, gamma=10, milestones=[10, 30, 60])\n",
    "\n",
    "def get_noise_dataset(shape=(1, 128, 256), target_id=0, num_samples=6000):\n",
    "    return NoiseDataset(shape, num_samples, target_id=target_id, noise_std=0.1)\n",
    "\n",
    "def get_donateacry_dataset():\n",
    "    return CryDataset(train_data_donateacry,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          # NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      target_transform=FixedValueTransform(value=1)\n",
    "                     )\n",
    "\n",
    "def get_esc50_train_dataset():\n",
    "    return CryDataset(train_data_esc50, \n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          # NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      # target_transform=ESC50LabelTransform()\n",
    "                      target_transform=BinaryTransform(20)\n",
    "                     )\n",
    "\n",
    "def get_esc50_valid_dataset():\n",
    "    return CryDataset(valid_data_esc50,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      #  target_transform=ESC50LabelTransform()\n",
    "                       target_transform=BinaryTransform(20)\n",
    "                     )\n",
    "\n",
    "def get_us8k_train_dataset():\n",
    "    return CryDataset(train_data_us8k, \n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                    #   target_transform=US8KLabelTransform()\n",
    "                     )\n",
    "\n",
    "def get_us8k_valid_dataset():\n",
    "    return CryDataset(valid_data_us8k,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                    #   target_transform=US8KLabelTransform()\n",
    "                     )\n",
    "def get_big_valid_dataset():\n",
    "    return ConcatDataset([get_esc50_valid_dataset(), get_us8k_valid_dataset()])\n",
    "\n",
    "train_loader = DataLoader(ConcatDataset([\n",
    "                                get_esc50_train_dataset(),\n",
    "                                get_donateacry_dataset(),\n",
    "                                # get_noise_dataset(num_samples=300)\n",
    "                            ]), \n",
    "                          batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(ConcatDataset([\n",
    "                                get_esc50_valid_dataset(),\n",
    "                            ]), \n",
    "                          batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "print(f'train_dataset:{len(train_loader.dataset)}')\n",
    "print(f'test_dataset:{len(test_loader.dataset)}')\n",
    "dump_dataset_info(train_loader.dataset)\n",
    "dump_dataset_info(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0555ac-f9d2-4360-a5f1-b4d5e7eca991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\condaenv\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120] LR:[0.001] Loss/train:3.4475 Loss/val:695.1843 Accuracy/val:2.90%\n",
      "Epoch [2/120] LR:[0.001] Loss/train:1.8363 Loss/val:6551.3565 Accuracy/val:2.07%\n",
      "Epoch [3/120] LR:[0.001] Loss/train:1.8870 Loss/val:5382.5084 Accuracy/val:2.07%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m     early_stopper \u001b[39m=\u001b[39m EarlyStopper(patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     49\u001b[0m model \u001b[39m=\u001b[39m ModelProxy(model)\u001b[39m.\u001b[39mset_file_pattern(model_file)\u001b[39m.\u001b[39mset_epoch(start_epoch)\u001b[39m.\u001b[39mload_if_checkpoint_exists()\u001b[39m.\u001b[39mset_auto_save([\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m], [])\n\u001b[1;32m---> 51\u001b[0m train(model, criterion, scheduler, start_epoch, end_epoch, train_loader, test_loader, writer\u001b[39m=\u001b[39;49mwriter, grad_scaler\u001b[39m=\u001b[39;49mgrad_scaler, l2_reg\u001b[39m=\u001b[39;49ml2_reg)\n\u001b[0;32m     52\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\jupyter\\cry-detection\\train.py:78\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model_saver, criterion, lr_scheduler, start_epoch, end_epoch, train_loader, test_loader, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m early_stopper \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mearly_stopper\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mearly_stopper\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, end_epoch):\n\u001b[1;32m---> 78\u001b[0m     epoch_loss \u001b[39m=\u001b[39m train_epoch(model, criterion, optimizer, train_loader)\n\u001b[0;32m     79\u001b[0m     avg_loss \u001b[39m=\u001b[39m epoch_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)    \n\u001b[0;32m     80\u001b[0m     val_loss, val_accuracy \u001b[39m=\u001b[39m evalute(model, criterion, test_loader)\n",
      "File \u001b[1;32md:\\jupyter\\cry-detection\\train.py:45\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, criterion, optimizer, dataloader, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m# 前向过程开启 autocast\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 45\u001b[0m     logits \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     46\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits, targets)\n\u001b[0;32m     48\u001b[0m \u001b[39m# 添加 L2 正则化\u001b[39;00m\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\jupyter\\cry-detection\\model\\mobilenet_v3.py:115\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 115\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m    116\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(out)\n\u001b[0;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\jupyter\\cry-detection\\model\\mobilenet_v3.py:58\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 58\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[0;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_res_connect:\n\u001b[0;32m     60\u001b[0m         out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m x\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "# 混合精度\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystop import EarlyStopper\n",
    "from utils.modelproxy import ModelProxy\n",
    "from utils.focalloss import FocalLoss\n",
    "\n",
    "from model.mobilenet_v3 import MobileNetV3\n",
    "from model.mobilenet_v2 import MobileNetV2\n",
    "from train import train\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = MobileNetV2((1, 128, 100), 21).to(device)\n",
    "model = MobileNetV3((1, 128, 256), classes, width_multiplier=1.0, dropout_rate=0.2).to(device)\n",
    "# print(model)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "with_tqdm = False\n",
    "with_early_stop = False\n",
    "with_l2_regularization = True\n",
    "with_grad_scaler = True\n",
    "start_epoch = 0\n",
    "end_epoch = 120\n",
    "learning_rate = 1e-3\n",
    "save_model_start = 10\n",
    "save_model_period = 10\n",
    "l2_reg = 0.05\n",
    "\n",
    "model_name = f'mobilenetv3-binary'\n",
    "model_file = f'./weights/{model_name}/epoch_{{epoch:03d}}.pt'\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(classes, alpha=0.75, gamma=2.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
    "grad_scaler = GradScaler() if with_grad_scaler else None\n",
    "writer = SummaryWriter()\n",
    "\n",
    "if with_early_stop:\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "model = ModelProxy(model).set_file_pattern(model_file).set_epoch(start_epoch).set_auto_save([10, 5], [])\n",
    "model.load_checkpoint()\n",
    "\n",
    "train(model, criterion, scheduler, start_epoch, end_epoch, train_loader, test_loader, writer=writer, grad_scaler=grad_scaler, l2_reg=l2_reg)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69597e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "waveform, sr = torchaudio.load('E:/dataset/ESC-50-master/audio/1-100032-A-0.wav')\n",
    "resample = torchaudio.transforms.Resample(sr, 8000)\n",
    "spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=8000,\n",
    "    n_fft=512,\n",
    "    win_length=20,\n",
    "    hop_length=10, \n",
    "    n_mels=128)\n",
    "\n",
    "eps = torch.Tensor([1e-6])\n",
    "# spec = spec.numpy()\n",
    "# spec = np.log(spec + eps)\n",
    "\n",
    "\n",
    "out = spectrogram(resample(waveform[:44100]))\n",
    "out += eps\n",
    "out = out.log()\n",
    "# time = len(out[0]) * 1000 / 8000\n",
    "# print(time)\n",
    "# print(out.shape)\n",
    "\n",
    "plt.pcolormesh(out[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c66696-84b2-4d5b-a87b-6aa328fede55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def get_spec(waveform, sampling_rate=24000, n_fft=512, window_length=20, hop_length=10):\n",
    "\tspecs = []\n",
    "\twaveform = torch.Tensor(waveform)\n",
    "\ttransform = torchaudio.transforms.MelSpectrogram(\n",
    "\t\tsample_rate=sampling_rate, \n",
    "\t\tn_fft=n_fft, \n",
    "\t\twin_length=window_length, \n",
    "\t\thop_length=hop_length, \n",
    "\t\tn_mels=128)\n",
    "\tspec = transform(waveform)\n",
    "\teps = 1e-6\n",
    "\tspec = spec.numpy()\n",
    "\tspec = np.log(spec + eps)\n",
    "\tx_min = spec.min()\n",
    "\tx_max = spec.max()\n",
    "\tspec = (spec - x_min) / (x_max - x_min)\n",
    "\tfor j in range(0, spec.shape[1] - 51, 50):\n",
    "\t\tslice = spec[:, j:j+100]\n",
    "\t\t# print(f'slice shape: {slice.shape}, range: {j}:{j+100}')\n",
    "\t\tspecs.append(slice)\n",
    "\treturn specs\n",
    "\n",
    "def extract_spectrogram(values, clip, entries):\n",
    "\tfor data in entries:\n",
    "\n",
    "\t\tnum_channels = 2\n",
    "\t\twindow_sizes = [20, 40]\n",
    "\t\thop_sizes = [10, 20]\n",
    "\t\t# window_sizes = [20]\n",
    "\t\t# hop_sizes = [10]\n",
    "\n",
    "\t\tspecs = []\n",
    "\t\tfor i in range(num_channels):\n",
    "\t\t\twindow_length = int(round(window_sizes[i]*args.sampling_rate/1000))\n",
    "\t\t\thop_length = int(round(hop_sizes[i]*args.sampling_rate/1000))\n",
    "\n",
    "\t\t\tclip = torch.Tensor(clip)\n",
    "\t\t\tspec = torchaudio.transforms.MelSpectrogram(sample_rate=args.sampling_rate, n_fft=512, win_length=window_length, hop_length=hop_length, n_mels=128)(clip)\n",
    "\t\t\teps = 1e-6\n",
    "\t\t\tspec = spec.numpy()\n",
    "\t\t\tspec = np.log(spec + eps)\n",
    "\t\t\t# print(f'channel: {i} shape: {spec.shape}')\n",
    "\t\t\tfor j in range(0, spec.shape[1] - 51, 50):\n",
    "\t\t\t\tslice = spec[:, j:j+100]\n",
    "\t\t\t\t# print(f'slice shape: {slice.shape}, range: {j}:{j+100}')\n",
    "\t\t\t\tspecs.append(slice)\n",
    "\t\t\t# print(spec.shape)\n",
    "\t\t\t# spec = np.asarray(torchvision.transforms.Resize((128, 250))(Image.fromarray(spec)))\n",
    "\t\t\t# specs.append(spec)\n",
    "\t\tnew_entry = {}\n",
    "\t\t# new_entry[\"audio\"] = clip.numpy()\n",
    "\t\tnew_entry[\"values\"] = np.array(specs)\n",
    "\t\tnew_entry[\"target\"] = data[\"target\"]\n",
    "\t\tvalues.append(new_entry)\n",
    "\n",
    "clip, sr = librosa.load(\"d:\\\\code\\\\jupyter\\\\audio\\\\positive\\\\baby_cry_16bit_8k.wav\", sr=24000)\n",
    "# clip, sr = librosa.load(\"E:\\\\dataset\\\\bilibili\\\\cry2.m4s\", sr=24000)\n",
    "print(clip.shape, sr)\n",
    "clip = clip[:len(clip) // 1000 * 1000]\n",
    "print(clip.shape)\n",
    "# entries = audios.loc[audios[\"filename\"]==audio].to_dict(orient=\"records\")\n",
    "values = get_spec(clip, sampling_rate=sr)\n",
    "values = [np.expand_dims(value, 0) for value in values[:1000]]\n",
    "values = torch.Tensor(values).to(device)\n",
    "print(values.size())\n",
    "# print(len(values))\n",
    "# print(values[0].shape)\n",
    "\n",
    "predict = model(values).detach().cpu().numpy()\n",
    "predict = [np.argmax(p) for p in predict]\n",
    "print(predict)\n",
    "# print(torch.argmax(predict[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

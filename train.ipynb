{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20f395-b21c-4a62-9d8d-c3c17a00319c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/karoldvl/ESC-50/archive/master.zip\n",
    "# !mkdir -p data && cd data && unzip ../master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dd4900-bbd6-41da-864a-c30a0a568f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:17837\n",
      "test_dataset:2894\n",
      "dataset info\n",
      "records: 17837\n",
      "classes: 2\n",
      "class[0] items: 11360\n",
      "class[1] items: 6477\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from utils.transform import UnsqueezeTransform, NoiseGeneratorTransform, FixedValueTransform, BinaryTransform\n",
    "from utils.dataset import dump_dataset_info\n",
    "from dataset.cry import CryDataset\n",
    "from dataset.noise import NoiseDataset\n",
    "\n",
    "\n",
    "# 定义超参数\n",
    "classes = 2\n",
    "train_batch_size = 300\n",
    "test_batch_size = 5\n",
    "train_data_esc50 = 'e:/dataset/out/esc50pp/training128mel1.pkl'\n",
    "valid_data_esc50 = 'e:/dataset/out/esc50pp/validation128mel1.pkl'\n",
    "train_data_donateacry = 'e:/dataset/out/dnac/donateacry.pkl'\n",
    "\n",
    "train_data_us8k = 'e:/dataset/out/us8k/training128mel1.pkl'\n",
    "valid_data_us8k = 'e:/dataset/out/us8k/validation128mel1.pkl'\n",
    "\n",
    "noise = NoiseGeneratorTransform(noise_std=1e-6, gamma=10, milestones=[10, 30, 60])\n",
    "\n",
    "def get_noise_dataset(shape=(1, 128, 256), target_id=0, num_samples=6000):\n",
    "    return NoiseDataset(shape, num_samples, target_id=target_id, noise_std=0.1)\n",
    "\n",
    "def get_donateacry_dataset():\n",
    "    return CryDataset(train_data_donateacry,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          # NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      target_transform=FixedValueTransform(value=1)\n",
    "                     )\n",
    "\n",
    "def get_esc50_train_dataset():\n",
    "    return CryDataset(train_data_esc50, \n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          # NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      # target_transform=ESC50LabelTransform()\n",
    "                      target_transform=BinaryTransform(20)\n",
    "                     )\n",
    "\n",
    "def get_esc50_valid_dataset():\n",
    "    return CryDataset(valid_data_esc50,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                      #  target_transform=ESC50LabelTransform()\n",
    "                       target_transform=BinaryTransform(20)\n",
    "                     )\n",
    "\n",
    "def get_us8k_train_dataset():\n",
    "    return CryDataset(train_data_us8k, \n",
    "                      transform=torch.nn.Sequential(\n",
    "                          noise, \n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                    #   target_transform=US8KLabelTransform()\n",
    "                     )\n",
    "\n",
    "def get_us8k_valid_dataset():\n",
    "    return CryDataset(valid_data_us8k,\n",
    "                      transform=torch.nn.Sequential(\n",
    "                          #  NormalizeTransform(),\n",
    "                          UnsqueezeTransform(),\n",
    "                      ), \n",
    "                    #   target_transform=US8KLabelTransform()\n",
    "                     )\n",
    "def get_big_valid_dataset():\n",
    "    return ConcatDataset([get_esc50_valid_dataset(), get_us8k_valid_dataset()])\n",
    "\n",
    "train_loader = DataLoader(ConcatDataset([\n",
    "                                get_esc50_train_dataset(),\n",
    "                                get_donateacry_dataset(),\n",
    "                                # get_noise_dataset(num_samples=300)\n",
    "                            ]), \n",
    "                          batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(ConcatDataset([\n",
    "                                get_esc50_valid_dataset(),\n",
    "                            ]), \n",
    "                          batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "print(f'train_dataset:{len(train_loader.dataset)}')\n",
    "print(f'test_dataset:{len(test_loader.dataset)}')\n",
    "dump_dataset_info(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0555ac-f9d2-4360-a5f1-b4d5e7eca991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\condaenv\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/120] LR:[0.001] Loss/train:9.2719 Loss/val:0.10212056680666923% Accuracy/val:97.72%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m\n\u001b[0;32m     57\u001b[0m     early_stopper \u001b[39m=\u001b[39m EarlyStopper(patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m     59\u001b[0m saver \u001b[39m=\u001b[39m ModelSaver(model, model_file, \u001b[39mTrue\u001b[39;00m, start_epoch, [\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m], [])\n\u001b[1;32m---> 61\u001b[0m train(saver, criterion, scheduler, start_epoch, end_epoch, train_loader, test_loader, writer, grad_scaler)\n",
      "File \u001b[1;32md:\\jupyter\\cry-detection\\train.py:71\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model_saver, criterion, lr_scheduler, start_epoch, end_epoch, train_loader, test_loader, writer, grad_scaler)\u001b[0m\n\u001b[0;32m     69\u001b[0m optimizer \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39moptimizer\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, end_epoch):\n\u001b[1;32m---> 71\u001b[0m     epoch_loss \u001b[39m=\u001b[39m train_epoch(model, criterion, optimizer, train_loader, grad_scaler)\n\u001b[0;32m     72\u001b[0m     \u001b[39m# if (epoch + 1) >= save_model_start and (epoch + 1) % save_model_period == 0:\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39m#     torch.save(model.state_dict(), model_pattern.format(epoch=epoch + 1))\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m     \u001b[39m# train_acc = evalute(model, train_loader)        \u001b[39;00m\n\u001b[0;32m     76\u001b[0m     avg_loss \u001b[39m=\u001b[39m epoch_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)    \n",
      "File \u001b[1;32md:\\jupyter\\cry-detection\\train.py:56\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, criterion, optimizer, dataloader, grad_scaler)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m grad_scaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     grad_scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 56\u001b[0m     grad_scaler\u001b[39m.\u001b[39;49mstep(optimizer)\n\u001b[0;32m     57\u001b[0m     grad_scaler\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    372\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 374\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n\u001b[0;32m    378\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:290\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39msum\u001b[39m(v\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m--> 290\u001b[0m     retval \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mf:\\condaenv\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py:505\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(max_exp_avg_sq_sqrt, eps)\n\u001b[0;32m    504\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 505\u001b[0m     exp_avg_sq_sqrt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[0;32m    506\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    507\u001b[0m     denom \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_foreach_add(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "# 混合精度\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.earlystop import EarlyStopper\n",
    "from utils.modelsaver import ModelSaver\n",
    "\n",
    "from model.mobilenet_v3 import MobileNetV3\n",
    "from model.mobilenet_v2 import MobileNetV2\n",
    "from train import train\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = AudioClassifier(w=256, h=128, classes=21, num_conv_layers=3).to(device)\n",
    "# model = MobileNetV2((1, 128, 100), 21).to(device)\n",
    "model = MobileNetV3((1, 128, 256), classes, width_multiplier=1.0, dropout_rate=0.2).to(device)\n",
    "# print(model)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "with_tqdm = False\n",
    "with_early_stop = False\n",
    "with_l2_regularization = True\n",
    "with_grad_scaler = True\n",
    "start_epoch = 30\n",
    "end_epoch = 120\n",
    "learning_rate = 1e-3\n",
    "save_model_start = 10\n",
    "save_model_period = 10\n",
    "l2_reg = 0.05\n",
    "\n",
    "model_name = f'mobilenetv3-binary'\n",
    "model_file = f'./weights/{model_name}/epoch_{{epoch:03d}}.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
    "grad_scaler = GradScaler() if with_grad_scaler else None\n",
    "writer = SummaryWriter()\n",
    "\n",
    "if with_early_stop:\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "saver = ModelSaver(model, model_file, True, start_epoch, [10, 5], [])\n",
    "\n",
    "train(saver, criterion, scheduler, start_epoch, end_epoch, train_loader, test_loader, writer=writer, grad_scaler=grad_scaler, l2_reg=l2_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.load_state_dict(torch.load('./model_epoch_40.pt'))\n",
    "print(f'Final Test Accuracy: {evalute(model, test_loader):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69597e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "waveform, sr = torchaudio.load('E:/dataset/ESC-50-master/audio/1-100032-A-0.wav')\n",
    "resample = torchaudio.transforms.Resample(sr, 8000)\n",
    "spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=8000,\n",
    "    n_fft=512,\n",
    "    win_length=20,\n",
    "    hop_length=10, \n",
    "    n_mels=128)\n",
    "\n",
    "eps = torch.Tensor([1e-6])\n",
    "# spec = spec.numpy()\n",
    "# spec = np.log(spec + eps)\n",
    "\n",
    "\n",
    "out = spectrogram(resample(waveform[:44100]))\n",
    "out += eps\n",
    "out = out.log()\n",
    "# time = len(out[0]) * 1000 / 8000\n",
    "# print(time)\n",
    "# print(out.shape)\n",
    "\n",
    "plt.pcolormesh(out[0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91dabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from AudioClassifier import AudioClassifier\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AudioClassifier(w=128, h=100, classes=21, num_conv_layers=3).to(device)\n",
    "model.load_state_dict(torch.load('models/audioclassifier3/epoch_100.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c66696-84b2-4d5b-a87b-6aa328fede55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def get_spec(waveform, sampling_rate=24000, n_fft=512, window_length=20, hop_length=10):\n",
    "\tspecs = []\n",
    "\twaveform = torch.Tensor(waveform)\n",
    "\ttransform = torchaudio.transforms.MelSpectrogram(\n",
    "\t\tsample_rate=sampling_rate, \n",
    "\t\tn_fft=n_fft, \n",
    "\t\twin_length=window_length, \n",
    "\t\thop_length=hop_length, \n",
    "\t\tn_mels=128)\n",
    "\tspec = transform(waveform)\n",
    "\teps = 1e-6\n",
    "\tspec = spec.numpy()\n",
    "\tspec = np.log(spec + eps)\n",
    "\tx_min = spec.min()\n",
    "\tx_max = spec.max()\n",
    "\tspec = (spec - x_min) / (x_max - x_min)\n",
    "\tfor j in range(0, spec.shape[1] - 51, 50):\n",
    "\t\tslice = spec[:, j:j+100]\n",
    "\t\t# print(f'slice shape: {slice.shape}, range: {j}:{j+100}')\n",
    "\t\tspecs.append(slice)\n",
    "\treturn specs\n",
    "\n",
    "def extract_spectrogram(values, clip, entries):\n",
    "\tfor data in entries:\n",
    "\n",
    "\t\tnum_channels = 2\n",
    "\t\twindow_sizes = [20, 40]\n",
    "\t\thop_sizes = [10, 20]\n",
    "\t\t# window_sizes = [20]\n",
    "\t\t# hop_sizes = [10]\n",
    "\n",
    "\t\tspecs = []\n",
    "\t\tfor i in range(num_channels):\n",
    "\t\t\twindow_length = int(round(window_sizes[i]*args.sampling_rate/1000))\n",
    "\t\t\thop_length = int(round(hop_sizes[i]*args.sampling_rate/1000))\n",
    "\n",
    "\t\t\tclip = torch.Tensor(clip)\n",
    "\t\t\tspec = torchaudio.transforms.MelSpectrogram(sample_rate=args.sampling_rate, n_fft=512, win_length=window_length, hop_length=hop_length, n_mels=128)(clip)\n",
    "\t\t\teps = 1e-6\n",
    "\t\t\tspec = spec.numpy()\n",
    "\t\t\tspec = np.log(spec + eps)\n",
    "\t\t\t# print(f'channel: {i} shape: {spec.shape}')\n",
    "\t\t\tfor j in range(0, spec.shape[1] - 51, 50):\n",
    "\t\t\t\tslice = spec[:, j:j+100]\n",
    "\t\t\t\t# print(f'slice shape: {slice.shape}, range: {j}:{j+100}')\n",
    "\t\t\t\tspecs.append(slice)\n",
    "\t\t\t# print(spec.shape)\n",
    "\t\t\t# spec = np.asarray(torchvision.transforms.Resize((128, 250))(Image.fromarray(spec)))\n",
    "\t\t\t# specs.append(spec)\n",
    "\t\tnew_entry = {}\n",
    "\t\t# new_entry[\"audio\"] = clip.numpy()\n",
    "\t\tnew_entry[\"values\"] = np.array(specs)\n",
    "\t\tnew_entry[\"target\"] = data[\"target\"]\n",
    "\t\tvalues.append(new_entry)\n",
    "\n",
    "clip, sr = librosa.load(\"d:\\\\code\\\\jupyter\\\\audio\\\\positive\\\\baby_cry_16bit_8k.wav\", sr=24000)\n",
    "# clip, sr = librosa.load(\"E:\\\\dataset\\\\bilibili\\\\cry2.m4s\", sr=24000)\n",
    "print(clip.shape, sr)\n",
    "clip = clip[:len(clip) // 1000 * 1000]\n",
    "print(clip.shape)\n",
    "# entries = audios.loc[audios[\"filename\"]==audio].to_dict(orient=\"records\")\n",
    "values = get_spec(clip, sampling_rate=sr)\n",
    "values = [np.expand_dims(value, 0) for value in values[:1000]]\n",
    "values = torch.Tensor(values).to(device)\n",
    "print(values.size())\n",
    "# print(len(values))\n",
    "# print(values[0].shape)\n",
    "\n",
    "predict = model(values).detach().cpu().numpy()\n",
    "predict = [np.argmax(p) for p in predict]\n",
    "print(predict)\n",
    "# print(torch.argmax(predict[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc942ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'audio length: {7659000 / 8000}s')\n",
    "# print(7659000 / 1990)\n",
    "print(predict.index(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

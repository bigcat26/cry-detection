{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from model.mobilenet_v3 import MobileNetV3\n",
    "# from model.mobilenet_v2 import MobileNetV2\n",
    "# from model.AudioClassifier import AudioClassifier\n",
    "\n",
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x - c)\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "    y = exp_x / sum_exp_x\n",
    "    return y\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_file = './weights/class50/epoch_100.pt'\n",
    "\n",
    "# model = AudioClassifier(w=256, h=128, classes=21, num_conv_layers=3).to(device)\n",
    "# model = MobileNetV2((1, 128, 100), 21).to(device)\n",
    "model = MobileNetV3((1, 128, 256), 50, width_multiplier=1.0, dropout_rate=0).to(device)\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:e:/dataset/BabyCryDetectorSamples/1.wav slices: 16 confs len:16 class:[20 26 26 26 26 26 26 26 20 26 26 26 26 26 26 26]\n",
      "file:e:/dataset/BabyCryDetectorSamples/2.wav slices: 8 confs len:8 class:[5]\n",
      "file:e:/dataset/BabyCryDetectorSamples/3.wav slices: 5 confs len:5 class:[ 0 26  5  5]\n",
      "file:e:/dataset/BabyCryDetectorSamples/4.wav slices: 17 confs len:17 class:[ 6  6  6  6  6  6 20  6  6  6  2]\n",
      "file:e:/dataset/BabyCryDetectorSamples/5.wav slices: 12 confs len:12 class:[20 20 20  2  2  6]\n",
      "file:e:/dataset/BabyCryDetectorSamples/6.wav slices: 30 confs len:30 class:[ 2 20 20 20 20  6  6  6  6 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/7.wav slices: 26 confs len:26 class:[20 20 20 20 20  5 20 20 20 20 20 20 26 20 20 20  5 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/8.wav slices: 21 confs len:21 class:[ 9  9 20 20  6  5  0 26 26 20  9]\n",
      "file:e:/dataset/BabyCryDetectorSamples/9.wav slices: 28 confs len:28 class:[42  6  6 20 20 20 20 20 20 20 20  6  2  2 20 20 20  5]\n",
      "file:e:/dataset/BabyCryDetectorSamples/10.wav slices: 8 confs len:8 class:[20 20 20  6  6 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/11.wav slices: 13 confs len:13 class:[20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/12.wav slices: 27 confs len:27 class:[20 20 20 20 20 20  5 20 20 20 20 20 20 20 20 20  5 20 20 20 20 20 20 20\n",
      " 20 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/13.wav slices: 7 confs len:7 class:[20 20 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/14.wav slices: 49 confs len:49 class:[20 20 20 20 15 15 15 20 20  5 15 15 15  5 15  5 20 20 20  5  5 15  5 15\n",
      "  5 20 20 20 20  5 15 15 15 15 15 15  5 15 15 29  5 15 15]\n",
      "file:e:/dataset/BabyCryDetectorSamples/15.wav slices: 1 confs len:1 class:[]\n",
      "file:e:/dataset/BabyCryDetectorSamples/16.wav slices: 29 confs len:29 class:[15 20 20 15 15 20 20 20 20 20 20 20 20 20 20 20 20 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/17.wav slices: 95 confs len:95 class:[19 16  9  9  9 23  9  9  9  9  9  9 16 43  9 23 23  9  9 23  9 23  3 16\n",
      " 19 19 16 16 16 43 43 23 23 23  3 43 43 43 43 43  3 23 23 23 23 23  9  9\n",
      "  9 23 23 43 43 23 23 23 23  9  9  9  9 28  3  3  3  3  2  6 26 26 23]\n",
      "file:e:/dataset/BabyCryDetectorSamples/18.wav slices: 3 confs len:3 class:[5]\n",
      "file:e:/dataset/BabyCryDetectorSamples/19.wav slices: 5 confs len:5 class:[20 20 20  6  6]\n",
      "file:e:/dataset/BabyCryDetectorSamples/20.wav slices: 80 confs len:80 class:[20 20  5 20  5 15 20 20  5  5  5 20 20 20  5  5 20 20 20 20  5 20 20 20\n",
      " 15 15 15  6 15  5 20 20 20 20  5  5  6 20 20 20  5  5 15 15 15  5 20 20\n",
      " 20  6  6 29 20 29 20 20 15 15 15 15 15 20 20 20 20 20]\n",
      "file:e:/dataset/BabyCryDetectorSamples/21.wav slices: 6 confs len:6 class:[18 18 18 18 18 18]\n",
      "file:e:/dataset/BabyCryDetectorSamples/26.wav slices: 3 confs len:3 class:[]\n",
      "file:e:/dataset/BabyCryDetectorSamples/27.wav slices: 4 confs len:4 class:[38  0]\n",
      "file:e:/dataset/BabyCryDetectorSamples/28.wav slices: 2 confs len:2 class:[]\n",
      "file:e:/dataset/BabyCryDetectorSamples/29.wav slices: 2 confs len:2 class:[15]\n",
      "file:e:/dataset/BabyCryDetectorSamples/30.wav slices: 3 confs len:3 class:[6]\n",
      "file:e:/dataset/BabyCryDetectorSamples/31.wav slices: 2 confs len:2 class:[]\n",
      "file:e:/dataset/BabyCryDetectorSamples/32.wav slices: 4 confs len:4 class:[ 6 38 38 38]\n",
      "file:e:/dataset/BabyCryDetectorSamples/33.wav slices: 14 confs len:14 class:[5 5 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "from utils.audio import *\n",
    "\n",
    "def predict(model, file):\n",
    "    wav, sr = librosa.load(file, sr=24000)\n",
    "    wav, _ = wav_trim(wav)\n",
    "    spec = wav_to_spectrogram(torch.Tensor(wav), 512, 512, 256, 128)\n",
    "    slices = spectrogram_split(spec.detach().cpu().numpy(), [256], [128], 256)\n",
    "\n",
    "    # print(slices[0].shape)\n",
    "    # print(slices[1].shape)\n",
    "    result = []\n",
    "    for i in range(len(slices)):\n",
    "        x = torch.from_numpy(slices[i]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        result.append(softmax(model(x).detach().cpu().numpy()))\n",
    "\n",
    "    confs = np.array([np.max(m) for m in result])\n",
    "    label = np.array([np.argmax(m) for m in result])\n",
    "    label[confs < 0.4] = -1\n",
    "    \n",
    "    filtered_indices = np.where(label >= 0)\n",
    "    filtered_confs = confs[filtered_indices]\n",
    "    filtered_label = label[filtered_indices]\n",
    "    \n",
    "    # 计算每个值出现的次数\n",
    "    values, counts = np.unique(label, return_counts=True)\n",
    "\n",
    "    # 按出现次数从多到少排序\n",
    "    sorted_indices = np.argsort(-counts)\n",
    "    sorted_values = values[sorted_indices]\n",
    "    sorted_counts = counts[sorted_indices]\n",
    "    \n",
    "    cls = sorted_values[0]\n",
    "    cnf = sorted_counts[0] / len(label)\n",
    "\n",
    "    # result = [-1] * len(conf)\n",
    "    # result[conf > 0.6] = 1\n",
    "    # result = [np.argmax(m) for m in result]\n",
    "    print(f'file:{file} slices: {len(slices)} confs len:{len(confs)} class:{filtered_label}')\n",
    "\n",
    "for i in range(1, 34):\n",
    "    file = f'e:/dataset/BabyCryDetectorSamples/{i}.wav'\n",
    "    if os.path.exists(file):\n",
    "        predict(model, file)\n",
    "\n",
    "# predict(model, 'e:/dataset/testing/cry1.mp3')\n",
    "# predict(model, 'e:/dataset/testing/cry2.mp3')\n",
    "# predict(model, 'e:/dataset/testing/dogbark1.wav')\n",
    "# predict(model, 'e:/dataset/testing/shortbark.wav')\n",
    "# predict(model, 'e:/dataset/BabyCryDetectorSamples/30.wav')\n",
    "# predict(model, 'e:/dataset/BabyCryDetectorSamples/31.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'e:/dataset/testing/cry2.mp3'\n",
    "\n",
    "wav, sr = librosa.load(test_file, sr=24000)\n",
    "wav = wav_trim(wav)\n",
    "spec = wav_to_spectrogram(test_file, 24000, 512, 512, 256, 128)\n",
    "slices = spectrogram_split(spec[0].detach().cpu().numpy(), [256], [128], 256)\n",
    "\n",
    "print(len(slices))\n",
    "# print(slices[0].shape)\n",
    "# print(slices[1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

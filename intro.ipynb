{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53923a88-b9f6-4a91-b465-0516a44796ed",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "从现在开始我们讨论一些关于深度学习和语音识别，音频分类的话题，涉及到Spectrogram，MelSpectrogram，MFCC、pytorch、torchaudio等知识。如无特别说明，输入音频参数为16bit 44.1Khz，n_fft参数为400，hop length为200，编程语言为python，可以使用jupyter和matplotlib相关的包。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb605e-780b-4983-9d7b-193bfde4a849",
   "metadata": {},
   "source": [
    "https://pytorch.org/audio/stable/datasets.html#torchaudio.datasets.SPEECHCOMMANDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d5e2128-a4d5-458c-b9fb-0d0c7ec88f7e",
   "metadata": {},
   "source": [
    "一个[1,201,201]的tensor，其纬度分别为batch, n_freq, n_frame. 将其按n_frame分割为10个batch，最终得到[10, 208, 40]的tensor，其中n_frame从0开始，以20为步长，每次取40个。数据不足的部分以0填充。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1585c7e",
   "metadata": {},
   "source": [
    "一个音频事件数据集，每个音频都是时长5秒，若使用1秒窗口大小，0.5秒步距进行滑窗分割，则并非每一秒都包含音频事件的有效数据，即此5秒音频剪辑的开头、中间和结尾可能存在一些静音。在制作训练集的时候，应该使用整段5秒音频作为训练数据，还是使用窗口分割的音频作为训练数据，并排除掉安静的音频信号部分？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "059731f2",
   "metadata": {},
   "source": [
    "给我一个mobilenet的代码，在初始化时可以设置它的输入形状以及输出分类数目、卷积层数，除了这些以外还可以添加上其他可能用得上的超参数配置，并且设置好默认值\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2ca2ccb-ec61-4d83-9a11-ca920ff53d14",
   "metadata": {},
   "source": [
    "## MobileNet V3\n",
    "\n",
    "在上述代码中，我们首先定义了一些辅助函数，如conv_bn_act用于创建带有批标准化和ReLU激活函数的卷积层，conv_1x1_bn_act用于创建1x1的卷积层，SqueezeExcitation用于实现倾斜注意力模块。\n",
    "\n",
    "然后，我们定义了InvertedResidual类，用于创建MobileNetV3中的倒残差模块。在InvertedResidual类中，我们使用了上述辅助函数和SqueezeExcitation模块来构建模块的层。\n",
    "\n",
    "最后，我们定义了MobileNetV3类，其中包含了输入层、中间的Inverted Residual块和分类器。在初始化函数中，我们根据输入的参数设置超参数和网络层的结构。\n",
    "\n",
    "请注意，这只是MobileNetV3的一个简单实现示例，实际使用时可能需要根据具体任务和数据集进行调整和优化。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cf6e45d-22ae-4cdd-a57d-5b30996b2282",
   "metadata": {},
   "source": [
    "## MobileNet V2\n",
    "\n",
    "在上述代码中，我们定义了辅助函数conv_bn_act用于创建带有批标准化和ReLU激活函数的卷积层，以及InvertedResidual类用于创建MobileNetV2中的倒残差模块。\n",
    "\n",
    "在MobileNetV2类中，我们首先设置了一些超参数，并定义了输入层、中间的Inverted Residual块和分类器。初始化函数中，我们根据输入的参数设置超参数和网络层的结构。\n",
    "\n",
    "需要注意的是，这只是MobileNetV2的一个简单实现示例，实际使用时可能需要根据具体任务和数据集进行调整和优化。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b8d12e6",
   "metadata": {},
   "source": [
    "https://github.com/kamalesh0406/Audio-Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e789569-42f3-422a-bb7e-53cc4ed95805",
   "metadata": {},
   "source": [
    "计算声谱图不需要计算 MFCC。声谱图是一种二维图像，它显示了音频信号在时间和频率上的能量分布。它通常通过对音频信号进行短时傅里叶变换 (STFT) 来计算。\n",
    "\n",
    "STFT 将音频信号分成多个帧，然后对每个帧进行傅里叶变换，以计算每个帧的频谱。最后，将所有帧的频谱拼接在一起，就可以得到音频信号的声谱图。\n",
    "\n",
    "下面是一个示例代码，它展示了如何使用 STFT 来计算音频信号的声谱图并绘制图形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67460d3f-ffe3-401f-87a7-27ca75a7c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 waveform 是一个形状为 (1, 220500) 的张量，表示单通道音频\n",
    "# waveform = torch.randn(1, 220500)\n",
    "# sample_rate = 44100\n",
    "\n",
    "file = '/home/jovyan/data/ESC-50-master/audio/1-100032-A-0.wav'\n",
    "waveform, sample_rate = torchaudio.load(file, normalize=True)\n",
    "\n",
    "# 计算声谱图\n",
    "spectrogram_transform = torchaudio.transforms.Spectrogram()\n",
    "spectrogram = spectrogram_transform(waveform)\n",
    "\n",
    "print(f'spectrogram shape: {spectrogram.shape}')\n",
    "\n",
    "# 绘制声谱图\n",
    "plt.figure()\n",
    "plt.imshow(spectrogram[0].detach().log2().numpy(), cmap='gray', aspect='auto', origin='lower')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "magnitude = spectrogram[0].detach().log2().numpy()\n",
    "print(f'magnitude shape: {magnitude.shape}')\n",
    "step = 200\n",
    "frame_size = 400\n",
    "time = np.arange(magnitude.shape[-1]) * step / sample_rate\n",
    "freq = np.fft.rfftfreq(frame_size) * sample_rate\n",
    "plt.pcolormesh(time, freq, magnitude)\n",
    "plt.show()\n",
    "\n",
    "magnitude = spectrogram[0].detach().log10().numpy()\n",
    "print(f'magnitude shape: {magnitude.shape}')\n",
    "step = 200\n",
    "frame_size = 400\n",
    "time = np.arange(magnitude.shape[-1]) * step / sample_rate\n",
    "freq = np.fft.rfftfreq(frame_size) * sample_rate\n",
    "plt.pcolormesh(time, freq, magnitude, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffd3f13d-a97a-4136-951b-be4ec5ebb6bc",
   "metadata": {},
   "source": [
    "您可以使用卷积神经网络 (CNN) 来对声谱图进行声音特征提取并分类。CNN 是一种常用于图像分类和识别的神经网络，它可以自动学习图像中的局部模式和结构。由于声谱图本质上是一种二维图像，因此您可以使用 CNN 来对声谱图进行特征提取和分类。\n",
    "\n",
    "下面是一个简单的示例，它展示了如何使用 CNN 对声谱图进行声音特征提取并分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b2e9d-3188-4e93-8d70-d6d5e4491ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 假设您有一个数据加载器，它可以加载训练数据和标签\n",
    "# train_loader = ...\n",
    "\n",
    "# 定义 CNN 模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型并将其移动到 GPU（如果可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_classes=10).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(10):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{10}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef2f127a-697e-4617-a173-cf188c0c6403",
   "metadata": {},
   "source": [
    "Q: 在使用cnn进行实时音频特征提取分类的场景，若音频为8Khz采样，需要采集多长时间声音生成的声谱图才能有比较好的效果\n",
    "\n",
    "A: 在使用 CNN 进行实时音频特征提取和分类时，您需要采集足够长的音频来生成声谱图，以便获得比较好的效果。音频的长度取决于您要解决的问题和您的模型。\n",
    "\n",
    "通常，音频长度应足够长，以便包含足够的信息来进行分类。例如，如果您要对语音命令进行分类，则音频长度应至少为一个语音命令的长度。如果您要对音乐片段进行分类，则音频长度应至少为一个音乐短语的长度。\n",
    "\n",
    "此外，您还需要考虑您的模型的输入大小。如果您的模型需要固定大小的输入，则您需要确保生成的声谱图与模型的输入大小匹配。这可能需要调整 STFT 的参数，例如窗口大小和步幅。\n",
    "\n",
    "总之，您需要根据您要解决的问题和您的模型来确定音频的长度。建议您通过实验来确定最佳的音频长度。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "024091cb-f26f-453e-bb62-7df3c92195d4",
   "metadata": {},
   "source": [
    "Q: 声谱图特征能否用作唤醒词检测？\n",
    "\n",
    "A: 是的，声谱图特征可以用于唤醒词检测。唤醒词检测是指在连续的音频流中检测特定的唤醒词，以便激活语音助手或其他语音应用。这通常通过使用机器学习模型来实现，该模型可以学习唤醒词的声学特征，并在音频流中检测它们。\n",
    "\n",
    "声谱图是一种常用的音频特征，它可以描述音频信号在时间和频率上的能量分布。由于声谱图包含了音频信号的重要信息，因此它可以用作唤醒词检测模型的输入特征。\n",
    "\n",
    "在使用声谱图特征进行唤醒词检测时，您需要对音频流进行分帧处理，并对每个帧计算声谱图。然后，您可以将声谱图输入到唤醒词检测模型中，以判断当前帧是否包含唤醒词。\n",
    "\n",
    "总之，声谱图特征可以用于唤醒词检测。它可以描述音频信号在时间和频率上的能量分布，并为唤醒词检测模型提供重要的输入特征。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45622348",
   "metadata": {},
   "source": [
    "## pytorch的dataloader是否可以从两个或更多的dataset读取数据"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8e92e59",
   "metadata": {},
   "source": [
    "是的，PyTorch的DataLoader可以从两个或更多的数据集读取数据。你可以使用ConcatDataset类来实现这一点。它继承自Dataset类，可以将多个数据集连接在一起，然后传递给DataLoader进行读取1。\n",
    "\n",
    "例如，如果你想要同时读取MNIST、CIFAR10和CIFAR100三个数据集，你可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
    "\n",
    "mnist_data = MNIST('./data', train=True, download=True)\n",
    "cifar10_data = CIFAR10('./data', train=True, download=True)\n",
    "cifar100_data = CIFAR100('./data', train=True, download=True)\n",
    "\n",
    "concatenated_dataset = ConcatDataset([mnist_data, cifar10_data, cifar100_data])\n",
    "dataloader = DataLoader(concatenated_dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

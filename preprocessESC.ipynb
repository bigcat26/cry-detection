{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.audio import *\n",
    "\n",
    "# def spectrogram_split(spec, spec_win, spec_hop, spec_resize=-1):\n",
    "#     y, x = spec.shape\n",
    "#     slice = []\n",
    "#     n_frame = spec.shape[1]\n",
    "#     for i in range(len(spec_win)):\n",
    "#         win = spec_win[i]\n",
    "#         hop = spec_hop[i]\n",
    "#         # n_slice = (n_frame - win) // hop + 1\n",
    "#         # remain  = n_frame - (n_slice * hop - hop + win)\n",
    "#         # print(f'spec[{i}] win:{win} hop:{hop} n_slice:{n_slice} remain:{remain}')\n",
    "\n",
    "#         start = 0\n",
    "#         end = 0\n",
    "#         while end < n_frame:\n",
    "#             end = min(start + win, n_frame)\n",
    "#             # merge next frame if next frame < win // 2\n",
    "#             if n_frame - start - hop < win // 2:\n",
    "#                 end = n_frame\n",
    "            \n",
    "#             # print(f'  slice = [{start} - {end}] = {end - start}')\n",
    "#             img = Image.fromarray(spec[:, start:end] * 255)\n",
    "#             if spec_resize != end - start:\n",
    "#                 img = img.resize((spec_resize, y))\n",
    "#             slice.append(np.array(img))\n",
    "#             start += hop\n",
    "\n",
    "#     return slice\n",
    "\n",
    "# def wav_remove_silent(waveform, top_db=30, frame_length=2048, hop_length=512):\n",
    "#     non_silent_intervals = librosa.effects.split(waveform, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n",
    "#     return np.concatenate([waveform[start:end] for start, end in non_silent_intervals])\n",
    "\n",
    "# def wav_trim(waveform, top_db=30, frame_length=2048, hop_length=512):\n",
    "#     return librosa.effects.trim(waveform, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n",
    "\n",
    "# def wav_to_spectrogram(wav, n_fft, win_length, hop_length, n_mels=128):\n",
    "#     mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "#         n_fft=n_fft, \n",
    "#         win_length=win_length,\n",
    "#         hop_length=hop_length,\n",
    "#         n_mels=n_mels,\n",
    "#     )\n",
    "\n",
    "#     preprocess = torch.nn.Sequential(\n",
    "#         mel_spectrogram,\n",
    "#         Log2Transform(), \n",
    "#         NormalizeTransform(),\n",
    "#     )\n",
    "\n",
    "#     return preprocess(wav) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "us8k_config = {\n",
    "    'num_folds': 1,\n",
    "    'new_freq': 24000,\n",
    "    'n_fft': 512,\n",
    "    'hop_length': 256,\n",
    "    'win_length': 512,\n",
    "    'n_mels': 128,\n",
    "    'spec_win': [192, 256, 320],\n",
    "    'spec_hop': [96, 128, 160],\n",
    "    'spec_resize': 256,\n",
    "    'csv_file': 'E:/dataset/UrbanSound8K/metadata/UrbanSound8K.csv',\n",
    "    'data_dir': 'E:/dataset/UrbanSound8K/audio',\n",
    "    'store_dir': 'E:/dataset/out/us8k',\n",
    "    # 'csv_file': '../data/ESC-50-master/meta/esc50.csv',\n",
    "    # 'data_dir': '../data/ESC-50-master/audio',\n",
    "    # 'store_dir': './dataset',\n",
    "    'sampling_rate': 24000\n",
    "}\n",
    "\n",
    "def extract_features_us8k(config, audios):\n",
    "    audio_names = list(audios.slice_file_name.unique())\n",
    "    values = []\n",
    "    # print(f'records = {len(audios)}')\n",
    "    # print(f'records = {len(audio_names)}')\n",
    "    for file_name in tqdm(audio_names):\n",
    "        entries = audios.loc[audios[\"slice_file_name\"]==file_name].to_dict(orient=\"records\")[0]\n",
    "        full_path = f\"{config['data_dir']}/fold{entries['fold']}/{entries['slice_file_name']}\"\n",
    "        clip, sr = librosa.load(full_path, sr=config['new_freq'])\n",
    "        clip, _ = wav_trim(clip)\n",
    "        clip = wav_remove_silent(clip)\n",
    "        spec = wav_to_spectrogram(torch.Tensor(clip), config['n_fft'], config['win_length'], config['hop_length'], config['n_mels'])\n",
    "        slices = spectrogram_split(spec.numpy(), config['spec_win'], config['spec_hop'], config['spec_resize'])\n",
    "        entries = audios.loc[audios[\"filename\"]==file_name].to_dict(orient=\"records\")\n",
    "        target = entries[0]['target']\n",
    "        for v in slices:\n",
    "            values.append({'value': v, 'target': target})\n",
    "        # print(f\"Finished {file_name}\")\n",
    "        # print(f\"Finished: {full_path}\")\n",
    "    return values\n",
    "\n",
    "def preprocess_us8k(config):\n",
    "    audios = pd.read_csv(config['csv_file'], skipinitialspace=True)\n",
    "    num_folds = config['num_folds']\n",
    "\n",
    "    if not os.path.exists(config['store_dir']):\n",
    "        print(f\"creating directory: {config['store_dir']}\")\n",
    "        os.makedirs(config['store_dir'])\n",
    "\n",
    "    for i in range(1, num_folds+1):\n",
    "        training_audios = audios.loc[audios[\"fold\"]!=i]\n",
    "        validation_audios = audios.loc[audios[\"fold\"]==i]\n",
    "\n",
    "        print(f'Fold {i} training size: {len(training_audios)} validation size: {len(validation_audios)}')\n",
    "        training_values = extract_features_us8k(config, training_audios)\n",
    "        with open(f\"{config['store_dir']}/training128mel{i}.pkl\", \"wb\") as handler:\n",
    "            pkl.dump(training_values, handler, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "        validation_values = extract_features_us8k(config, validation_audios)\n",
    "        with open(f\"{config['store_dir']}/validation128mel{i}.pkl\", \"wb\") as handler:\n",
    "            pkl.dump(validation_values, handler, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "preprocess_us8k(us8k_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6377dcf7cf9541dca1a7dfbb36e24738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e08cf3672340e7bdf23a6fed0bbe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "esc50_config = {\n",
    "    'num_folds': 1,\n",
    "    'new_freq': 24000,\n",
    "    'n_fft': 512,\n",
    "    'hop_length': 256,\n",
    "    'win_length': 512,\n",
    "    'n_mels': 128,\n",
    "    'spec_win': [192, 256, 320],\n",
    "    'spec_hop': [96, 128, 160],\n",
    "    'spec_resize': 256,\n",
    "    'csv_file': 'E:/dataset/ESC-50-master/meta/esc50.csv',\n",
    "    'data_dir': 'E:/dataset/ESC-50-master/audio',\n",
    "    'store_dir': 'E:/dataset/out/esc50pp',\n",
    "    # 'csv_file': '../data/ESC-50-master/meta/esc50.csv',\n",
    "    # 'data_dir': '../data/ESC-50-master/audio',\n",
    "    # 'store_dir': './dataset',\n",
    "    'sampling_rate': 24000\n",
    "}\n",
    "\n",
    "def extract_features_esc50(config, audios):\n",
    "    audio_names = list(audios.filename.unique())\n",
    "    values = []\n",
    "    for file_name in tqdm(audio_names):\n",
    "        full_path = f\"{config['data_dir']}/{file_name}\"\n",
    "        clip, sr = librosa.load(full_path, sr=config['new_freq'])\n",
    "        clip, _ = wav_trim(clip)\n",
    "        clip = wav_remove_silent(clip)\n",
    "        spec = wav_to_spectrogram(torch.Tensor(clip), config['n_fft'], config['win_length'], config['hop_length'], config['n_mels'])\n",
    "        slices = spectrogram_split(spec.numpy(), config['spec_win'], config['spec_hop'], config['spec_resize'])\n",
    "        entries = audios.loc[audios[\"filename\"]==file_name].to_dict(orient=\"records\")\n",
    "        target = entries[0]['target']\n",
    "        for v in slices:\n",
    "            values.append({'value': v, 'target': target})\n",
    "        # print(f\"Finished {file_name} (target:{target})\")\n",
    "    return values\n",
    "\n",
    "def preprocess_esc50(config):\n",
    "    audios = pd.read_csv(config['csv_file'], skipinitialspace=True)\n",
    "    num_folds = config['num_folds']\n",
    "\n",
    "    if not os.path.exists(config['store_dir']):\n",
    "        print(f\"creating directory: {config['store_dir']}\")\n",
    "        os.makedirs(config['store_dir'])\n",
    "\n",
    "    for i in range(1, num_folds+1):\n",
    "        training_audios = audios.loc[audios[\"fold\"]!=i]\n",
    "        validation_audios = audios.loc[audios[\"fold\"]==i]\n",
    "\n",
    "        training_values = extract_features_esc50(config, training_audios)\n",
    "        # return training_values\n",
    "        with open(f\"{config['store_dir']}/training128mel{i}.pkl\", \"wb\") as handler:\n",
    "            pkl.dump(training_values, handler, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "        validation_values = extract_features_esc50(config, validation_audios)\n",
    "        with open(f\"{config['store_dir']}/validation128mel{i}.pkl\", \"wb\") as handler:\n",
    "            pkl.dump(validation_values, handler, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "preprocess_esc50(esc50_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing baby-cry-detection-ogg...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e75fec09b94ef99cd79994df3b9b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BabyCryDetectorSamples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe46c41e93cf4d5499feb7d8acb76b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing belly_pain...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f3872e25634309bf1ec795f8a05835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing burping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59900cd76bf04258887ab39c542b7ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing discomfort...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f367ef36f214797bcbed924492501f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hungry...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a957a0355b4b3283e9152e7065d384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tired...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e16dcf50771464ea69d9f6d64d140cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "dnac_config = {\n",
    "    'new_freq': 24000,\n",
    "    'n_fft': 512,\n",
    "    'hop_length': 256,\n",
    "    'win_length': 512,\n",
    "    'n_mels': 128,\n",
    "    'spec_win': [192, 256, 320],\n",
    "    'spec_hop': [96, 128, 160],\n",
    "    'spec_resize': 256,\n",
    "    'data_dir': 'E:/dataset/donate-a-cry-corpus',\n",
    "    'store_dir': 'E:/dataset/out/dnac',\n",
    "    'sampling_rate': 24000\n",
    "}\n",
    "\n",
    "def file_to_spectrogram(file, config):\n",
    "    clip, _ = librosa.load(file, sr=config['new_freq'])\n",
    "    clip, _ = wav_trim(clip)\n",
    "    clip = wav_remove_silent(clip)\n",
    "    spec = wav_to_spectrogram(torch.Tensor(clip), config['n_fft'], config['win_length'], config['hop_length'], config['n_mels'])\n",
    "    return spectrogram_split(spec.numpy(), config['spec_win'], config['spec_hop'], config['spec_resize'])\n",
    "\n",
    "def preprocess_donateacry(config):\n",
    "    target = 0\n",
    "    values = []\n",
    "\n",
    "    if not os.path.exists(config['store_dir']):\n",
    "        print(f\"creating directory: {config['store_dir']}\")\n",
    "        os.makedirs(config['store_dir'])\n",
    "\n",
    "    entries = os.listdir(config['data_dir'])\n",
    "    for entry in entries:\n",
    "        dirpath = f\"{config['data_dir']}/{entry}\"\n",
    "        if not os.path.isdir(dirpath):\n",
    "            continue\n",
    "        print(f\"Processing {entry}...\")\n",
    "        \n",
    "        files = os.listdir(dirpath)\n",
    "        for file in tqdm(files):\n",
    "            if not file.endswith('wav'):\n",
    "                continue\n",
    "            \n",
    "            full_path = f\"{dirpath}/{file}\"\n",
    "            slices = file_to_spectrogram(full_path, config)\n",
    "            for piece in slices:\n",
    "                values.append({'value': piece, 'target': target})\n",
    "            \n",
    "        target += 1\n",
    "    \n",
    "    with open(f\"{config['store_dir']}/donateacry.pkl\", \"wb\") as f:\n",
    "        pkl.dump(values, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "preprocess_donateacry(dnac_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from model.mobilenet_v3 import MobileNetV3\n",
    "# from model.mobilenet_v2 import MobileNetV2\n",
    "# from model.AudioClassifier import AudioClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 统计元素出现次数\n",
    "counter = Counter(result)\n",
    "\n",
    "# 按照出现次数从多到少排序\n",
    "sorted_items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 打印结果\n",
    "for item in sorted_items:\n",
    "    print(f\"{item[0]}: {item[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
